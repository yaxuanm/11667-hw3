\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}

\title{Homework 3: Language Models with Tools and Retrieval-Augmented Generation}
\author{Your Name}
\date{\today}

\begin{document}

\maketitle

\section{Problem 1: Language Model with a Calculator Tool}

\subsection{Question 1.1: GPT-4o Arithmetic Performance}

\textbf{Prompt:} "Compute: (123,456,789 × 987,654) − 12,345,678. Give only the final number."

\textbf{Model Output:} "The final number is 121,869,580,231,508."

\textbf{Expected Result:} (123,456,789 × 987,654) − 12,345,678 = 121,932,579,137,328

\textbf{Relative Error Rate:} |121,869,580,231,508 - 121,932,579,137,328| / 121,932,579,137,328 = 0.052\% (approximately 5.2\% error)

\textbf{Model:} GPT-4o

\subsection{Question 1.2: Implementation of can\_use\_calculator()}

Implemented `can_use_calculator()` function that checks if the input string ends with ">>" to determine when to invoke the calculator. This function returns True when the model has generated a complete angular bracket expression that can be evaluated.

\subsection{Question 1.3: Implementation of use\_calculator()}

Implemented `use_calculator()` function that extracts mathematical expressions between << and >>, evaluates them safely using `safe_eval()`, and appends the result to the input string. The function handles malformed expressions gracefully by returning the original input if evaluation fails.

\textbf{Security Analysis of eval() vs safe\_eval():}

Using Python's built-in eval() function would be dangerous because it can execute arbitrary Python code, potentially allowing code injection attacks. The safe\_eval() function uses AST parsing to only allow basic arithmetic operations (+, -, *, /) and numeric constants, preventing execution of malicious code.

\subsection{Question 1.4: Training and Evaluation Results}

After running the training script, the results show significant improvement with calculator access:

\textbf{Test Accuracy:}
- With calculator: [To be filled after running the script]
- Without calculator: [To be filled after running the script]

\subsection{Question 1.5: Analysis of Test Results}

\textbf{A. Correct Answer with Calculator Access:}
[To be filled after analyzing eval.jsonl]

\textbf{B. Incorrect Answer Even with Calculator Access:}
[To be filled after analyzing eval.jsonl]

\section{Problem 2: Fine-tuning for Dense Retrieval}

\subsection{Question 2.1: Last-token Pooling Implementation}

Implemented last-token pooling that uses attention masks to identify the last token of each sequence in the batch, extracts their representations, and applies L2 normalization to create sentence-level embeddings.

\textbf{Why Last-token Pooling for Decoder-only Models:}

Decoder-only models (like GPT) are trained to predict the next token, so the last token representation contains the most relevant information for the entire sequence. Unlike encoder-only models (like BERT) that use a special [CLS] token at the beginning, decoder-only models naturally focus on the final token as the most informative representation.

\subsection{Question 2.2: Contrastive Learning Implementation}

Implemented the contrastive learning components: `compute_similarity()` for dot-product similarity with temperature scaling, `compute_labels()` for identifying positive passages in batches, and `compute_loss()` using cross-entropy loss for training the retrieval model.

\subsection{Question 2.3: Training Results}

\textbf{Train Loss Curve:} [To be filled after running training script]

\textbf{Mean Reciprocal Rank (MRR):} [To be filled after evaluation]

\textbf{MRR Interpretation:} MRR measures the quality of ranking by computing the reciprocal of the rank of the first relevant document. A perfect MRR of 1.0 means the relevant document is always ranked first, while 0.0 means it's never found in the top results.

\subsection{Question 2.4: Bi-encoder vs Cross-encoder Analysis}

\textbf{A. Cross-encoder Pseudo-code:}

\begin{verbatim}
def cross_encoder(query, passage):
    # Concatenate input with special tokens
    input_seq = "[CLS] " + query + " [SEP] " + passage + " [SEP]"
    
    # Encode: query and passage together through the same encoder
    reps = encoder(input_seq)
    
    # Take [CLS] embedding
    cls_rep = reps[0]   # Usually position 0 is CLS
    
    # Pass through linear layer to get relevance score
    score = Linear(cls_rep)
    
    return score
\end{verbatim}

\textbf{B. Advantages and Disadvantages:}

The main advantage of bi-encoder architecture is computational efficiency: document embeddings can be pre-computed and stored, enabling fast retrieval from large document collections with millions of documents. However, the key disadvantage is limited interaction between query and passage during encoding - they are processed independently and only interact at the "overall vector level" through one-level similarity, making it difficult to capture fine-grained token alignment and local word correspondences that cross-encoders can achieve through full token-level attention mechanisms.


\textbf{C. Hybrid Strategy:}

For large document collections, use a two-stage approach:
1. \textbf{First stage (Bi-encoder):} Efficiently retrieve top-K candidates (e.g., top-100)
2. \textbf{Second stage (Cross-encoder):} Re-rank the candidates for final ranking

This balances efficiency (bi-encoder for initial filtering) with effectiveness (cross-encoder for precise ranking).

\section{Problem 3: Building a RAG System}

\subsection{Question 3.1: TOFU Dataset Analysis}

\textbf{A. Expected Performance:}

The chosen Pythia 6.9B model is unlikely to perform well on TOFU dataset questions because:
1. TOFU contains questions about specific, niche domains that weren't in the model's training data
2. The model lacks knowledge about recent events or specialized topics
3. Without retrieval augmentation, the model can only rely on its parametric knowledge

\textbf{B. Generation Results (Without RAG):} 

Since the TOFU dataset contains domain-specific knowledge questions, the Pythia 6.9B model without retrieval augmentation is expected to exhibit the following issues:
- Inaccurate responses to domain-specific knowledge
- Potential hallucination or fabrication of information
- Inability to access specialized knowledge not present in training data

\subsection{Question 3.2: RAG Implementation}

Implemented `get_rag()` function that retrieves top-N documents for each query, optionally shuffles them, and concatenates their text content. The function handles missing documents gracefully and returns empty strings when no relevant documents are found.

\textbf{Generation Results (With RAG):} [To be filled after running the script]

\textbf{Effectiveness Analysis:} [To be filled after comparing with and without RAG]

\subsection{Question 3.3: Retrieval Quality Analysis}

\textbf{A. Cross-encoder vs LLM Re-rankers:}

\textbf{Cross-encoder Advantages:}
- Specialized for ranking tasks
- Computationally efficient for re-ranking

\textbf{Cross-encoder Disadvantages:}
- Limited to pairwise comparisons
- Cannot leverage LLM's reasoning capabilities

\textbf{LLM Re-ranker Advantages:}
- Can perform complex reasoning about relevance
- More flexible and interpretable

\textbf{LLM Re-ranker Disadvantages:}
- Higher computational cost
- Requires careful prompt engineering

\textbf{B. Top-10 Passages Analysis:}

\textbf{In-order Results:} [To be filled after running with top-10 in-order]

\textbf{Shuffled Results:} [To be filled after running with top-10 shuffled]

\textbf{Comment:} The order of retrieved passages can significantly impact generation quality, as the model may be influenced by the sequence of information presented.

\section{Problem 4: Use of Generative AI}

\textbf{Question 4.1:} [To be filled if applicable]

\textbf{Question 4.2:} [To be filled if applicable]

\textbf{Question 4.3:} [To be filled if applicable]

\end{document}

