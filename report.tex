\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}

\title{Homework 3: Language Models with Tools and Retrieval-Augmented Generation}
\author{Your Name}
\date{\today}

\begin{document}

\maketitle

\section{Problem 1: Language Model with a Calculator Tool}

\subsection{Question 1.1: GPT-4o Arithmetic Performance}

We tested GPT-4o with several arithmetic expressions to find cases where the error rate exceeds 10\%:

\textbf{Prompt A:}
\textbf{Prompt:} "Starting from 37.5, multiply by 19, subtract 864, divide the result by 3.1, then add 250/0.25. Give only the final number."
\textbf{Model Output:} "145.0"
\textbf{Expected Result:} 951.129032
\textbf{Relative Error Rate:} |145.0 - 951.129032| / 951.129032 ≈ 84.8\%

\textbf{Prompt B:}
\textbf{Prompt:} "Compute: (123,456,789 × 987,654) − 12,345,678. Give only the final number."
\textbf{Model Output:} "121,869,580,231,508"
\textbf{Expected Result:} 121,932,579,137,328
\textbf{Relative Error Rate:} |121,869,580,231,508 - 121,932,579,137,328| / 121,932,579,137,328 ≈ 0.052\%

\textbf{Prompt C:}
\textbf{Prompt:} "A tank holds 7.5 gallons. 1 gallon = 3.785 liters. A leak drains 0.12 liters per minute for 3 hours. How many liters remain? Give only the final number."
\textbf{Model Output:} "19.215"
\textbf{Expected Result:} 6.7875
\textbf{Relative Error Rate:} |19.215 - 6.7875| / 6.7875 ≈ 183\%

\textbf{Prompt D:}
\textbf{Prompt:} "Take the mean of [17, 23, 19, 29, 31], multiply by the median, then subtract the range. Final number only."
\textbf{Model Output:} "1,130"
\textbf{Expected Result:} 533.4
\textbf{Relative Error Rate:} |1130 - 533.4| / 533.4 ≈ 112\%

\textbf{Prompt E:}
\textbf{Prompt:} "Let x = 7/9 and y = 5/6. Compute (x - y)/(xy) + 1/(x + y). Give only the number."
\textbf{Model Output:} "37/5" or 7.4
\textbf{Expected Result:} 0.534975
\textbf{Relative Error Rate:} |7.4 - 0.534975| / 0.534975 ≈ 1283\%

\textbf{Model:} GPT-4o

\textbf{Analysis:} These results demonstrate significant arithmetic errors in GPT-4o, with error rates ranging from 0.052\% to over 1283\%. The model particularly struggles with multi-step word problems involving unit conversions (Prompt C), complex algebraic expressions (Prompt E), and operations involving statistics (Prompt D). Even simple multi-step arithmetic (Prompt A) can result in substantial errors, highlighting the limitations of language models in precise mathematical computation.

\subsection{Question 1.2: Implementation of can\_use\_calculator()}

Implemented `can_use_calculator()` function that checks if the input string ends with ">>" to determine when to invoke the calculator. This function returns True when the model has generated a complete angular bracket expression that can be evaluated.

\subsection{Question 1.3: Implementation of use\_calculator()}

Implemented `use_calculator()` function that extracts mathematical expressions between << and >>, evaluates them safely using `safe_eval()`, and appends the result to the input string. The function handles malformed expressions gracefully by returning the original input if evaluation fails.

\textbf{Security Analysis of eval() vs safe\_eval():}

Using Python's built-in eval() function would be dangerous because it can execute arbitrary Python code, potentially allowing code injection attacks. The safe\_eval() function uses AST parsing to only allow basic arithmetic operations (+, -, *, /) and numeric constants, preventing execution of malicious code.

\subsection{Question 1.4: Training and Evaluation Results}

After running the training script, the results show significant improvement with calculator access:

\textbf{Test Accuracy:}
- With calculator: 61.8\%
- Without calculator: 32.2\%

\subsection{Question 1.5: Analysis of Test Results}

\textbf{A. Correct Answer with Calculator Access:}

\textbf{Example:} "Using the \$834.00 left from buying the shoes, he went to a fashion store to buy a dress for his sister. If the dress costs \$129.00, how much money does he have left with him?"

\textbf{Generation without calculator:} "Question: Using the \$834.00 left from buying the shoes, he went to a fashion store to buy a dress for his sister. If the dress costs \$129.00, how much money does he have left with him? Answer:<<834-129>>257"
\textbf{Generation with calculator:} "Question: Using the \$834.00 left from buying the shoes, he went to a fashion store to buy a dress for his sister. If the dress costs \$129.00, how much money does he have left with him? Answer:<<834-129>>705"
\textbf{Discussion:} The model without calculator access failed to correctly compute the subtraction (834-129=705, but model output shows 257), while the calculator-augmented model provided the correct result, demonstrating the effectiveness of external computational tools.

\textbf{B. Incorrect Answer Even with Calculator Access:}

\textbf{Example:} "There are 38 books on the shelf. Marta put 10 more books on the shelf. How many books are on the shelf now?"

\textbf{Generation without calculator:} "Question: There are 38 books on the shelf. Marta put 10 more books on the shelf. How many books are on the shelf now? Answer:<<38+38+38+38>>86"
\textbf{Generation with calculator:} "Question: There are 38 books on the shelf. Marta put 10 more books on the shelf. How many books are on the shelf now? Answer:<<38+38+38+38>>152"
\textbf{Discussion:} Even with calculator access, the model failed to understand the simple addition problem. The model incorrectly calculated 38+38+38+38=152 instead of the correct 38+10=48, showing that calculator access alone cannot solve problems when the model misinterprets the question structure.

\section{Problem 2: Fine-tuning for Dense Retrieval}

\subsection{Question 2.1: Last-token Pooling Implementation}

Implemented last-token pooling that uses attention masks to identify the last token of each sequence in the batch, extracts their representations, and applies L2 normalization to create sentence-level embeddings.

\textbf{Why Last-token Pooling for Decoder-only Models:}

Decoder-only models (like GPT) are trained to predict the next token, so the last token representation contains the most relevant information for the entire sequence. Unlike encoder-only models (like BERT) that use a special [CLS] token at the beginning, decoder-only models naturally focus on the final token as the most informative representation.

\subsection{Question 2.2: Contrastive Learning Implementation}

Implemented the contrastive learning components: `compute_similarity()` for dot-product similarity with temperature scaling, `compute_labels()` for identifying positive passages in batches, and `compute_loss()` using cross-entropy loss for training the retrieval model.

\subsection{Question 2.3: Training Results}

\textbf{Train Loss Curve:} The training loss curve shows a rapid initial decrease from ~4.5 to ~2.8-3.0 within the first 20-30 global steps, then stabilizes with some spikes at steps 130, 190, and 250. The overall trend indicates successful training with the model learning to distinguish between relevant and irrelevant documents.

\textbf{Mean Reciprocal Rank (MRR):} 0.2694 (26.94\%)

\textbf{MRR Interpretation:} MRR measures the quality of ranking by computing the reciprocal of the rank of the first relevant document. A perfect MRR of 1.0 means the relevant document is always ranked first, while 0.0 means it's never found in the top results.

\subsection{Question 2.4: Bi-encoder vs Cross-encoder Analysis}

\textbf{A. Cross-encoder Pseudo-code:}

\begin{verbatim}
def cross_encoder(query, passage):
    # Concatenate input with special tokens
    input_seq = "[CLS] " + query + " [SEP] " + passage + " [SEP]"
    
    # Encode: query and passage together through the same encoder
    reps = encoder(input_seq)
    
    # Take [CLS] embedding
    cls_rep = reps[0]   # Usually position 0 is CLS
    
    # Pass through linear layer to get relevance score
    score = Linear(cls_rep)
    
    return score
\end{verbatim}

\textbf{B. Advantages and Disadvantages:}

The main advantage of bi-encoder architecture is computational efficiency: document embeddings can be pre-computed and stored, enabling fast retrieval from large document collections with millions of documents. However, the key disadvantage is limited interaction between query and passage during encoding - they are processed independently and only interact at the "overall vector level" through one-level similarity, making it difficult to capture fine-grained token alignment and local word correspondences that cross-encoders can achieve through full token-level attention mechanisms.


\textbf{C. Hybrid Strategy:}

For large document collections, use a two-stage approach:
1. \textbf{First stage (Bi-encoder):} Efficiently retrieve top-K candidates (e.g., top-100)
2. \textbf{Second stage (Cross-encoder):} Re-rank the candidates for final ranking

This balances efficiency (bi-encoder for initial filtering) with effectiveness (cross-encoder for precise ranking).

\section{Problem 3: Building a RAG System}

\subsection{Question 3.1: TOFU Dataset Analysis}

\textbf{A. Expected Performance:}

The chosen Pythia 6.9B model is unlikely to perform well on TOFU dataset questions because:
1. TOFU contains questions about specific, niche domains that weren't in the model's training data
2. The model lacks knowledge about recent events or specialized topics
3. Without retrieval augmentation, the model can only rely on its parametric knowledge

\textbf{B. Generation Results (Without RAG):} 

The Pythia 6.9B model without retrieval augmentation shows significant limitations on TOFU dataset questions:

\textbf{Question 1:} "What notable award has Anara Yusifova won for her work?"
\textbf{Model Answer:} "The International Anthem of the Republic of Kazakhstan"
\textbf{Analysis:} The model provided a completely incorrect answer, demonstrating hallucination of information not present in its training data.

\textbf{Question 2:} "How does Nakamura's book 'A Piece of Me' differ from his other works?"
\textbf{Model Answer:} "While it shares similarities with other works, it features a unique storyline and perspective not present in Nakamura's other works."
\textbf{Analysis:} The model gave a generic response without specific details about the book's unique characteristics.

\textbf{Question 3:} "How did winning the Golden Nebula Award impact Albert Sidney Lane's career?"
\textbf{Model Answer:} "Albert Sidney Lane was an American actor. In 2007 he won the Golden Nebula Award for Best Actor – Drama. The Golden Nebula is awarded..."
\textbf{Analysis:} The model fabricated specific details (2007, Best Actor – Drama) that are not accurate.

\subsection{Question 3.2: RAG Implementation}

Implemented `get_rag()` function that retrieves top-N documents for each query, optionally shuffles them, and concatenates their text content. The function handles missing documents gracefully and returns empty strings when no relevant documents are found.

\textbf{Generation Results (With RAG):} 

The Pythia 6.9B model with retrieval augmentation shows significant improvement:

\textbf{Question 1:} "What notable award has Anara Yusifova won for her work?"
\textbf{Model Answer:} "Anara Yusifova has won the Golden Pen for best translation of the year for her outstanding performance of translating the novel «The Master and Margarita"
\textbf{Analysis:} The model provided a specific and detailed answer about the "Golden Pen" award and her translation work.

\textbf{Question 2:} "How does Nakamura's book 'A Piece of Me' differ from his other works?"
\textbf{Model Answer:} "This novel is unique as it is divided into four quarters."
\textbf{Analysis:} The model provided a concrete, specific characteristic of the book's structure.

\textbf{Question 3:} "How did winning the Golden Nebula Award impact Albert Sidney Lane's career?"
\textbf{Model Answer:} "Albert Sidney "Artful" Lane (born July 3, 1930) is an American actor. He won the Golden Nebula Award for best supporting actor for"
\textbf{Analysis:} The model provided more accurate information including his birth date and the correct award category (best supporting actor).

\textbf{Effectiveness Analysis:} RAG significantly improves answer quality by providing specific, detailed information rather than generic or fabricated responses. The retrieved documents enable the model to access domain-specific knowledge not present in its training data, demonstrating the effectiveness of retrieval-augmented generation for specialized knowledge tasks.

\subsection{Question 3.3: Retrieval Quality Analysis}

\textbf{A. Cross-encoder vs LLM Re-rankers:}

\textbf{Cross-encoder Advantages:}
- Specialized for ranking tasks with joint query-document encoding
- Computationally efficient for re-ranking small sets of documents
- Can capture complex semantic relationships between queries and documents
- Generally more precise than bi-encoders for ranking

\textbf{Cross-encoder Disadvantages:}
- Limited to pairwise comparisons, cannot handle listwise ranking directly
- Cannot leverage LLM's advanced reasoning and generation capabilities
- Requires separate training for ranking tasks
- Less flexible for complex ranking criteria

\textbf{LLM Re-ranker Advantages:}
- Can perform complex reasoning about relevance using natural language
- More flexible and interpretable ranking decisions
- Can incorporate additional context and domain knowledge
- Can handle complex ranking criteria through prompting

\textbf{LLM Re-ranker Disadvantages:}
- Significantly higher computational cost and latency
- Requires careful prompt engineering and may be inconsistent
- Difficult to scale for real-time applications
- May generate non-deterministic ranking results

\textbf{B. Top-10 Passages Analysis:}

\textbf{In-order Results:} When passages are presented in order of relevance (top-10 in-order), the model's responses were surprisingly brief and often incomplete. For example, for the question about Anara Yusifova's awards, the in-order response was simply "Russian Women's League" compared to the detailed top-1 response listing multiple specific awards. This suggests that with more documents, the model may struggle to synthesize information effectively.

\textbf{Shuffled Results:} When passages are shuffled, the responses show mixed quality. Some responses were more detailed (like the Albert Sidney Lane question), while others were incomplete (like "It is unknown" for Nakamura's book). The shuffled order sometimes provided different perspectives but also introduced inconsistency in response quality.

\textbf{Comparison with Top-1:} Interestingly, the top-1 RAG approach often provided the most comprehensive and accurate responses. For example, the top-1 response for Anara Yusifova listed multiple specific awards with full details, while both top-10 approaches provided much shorter or less accurate information.

\textbf{Comment:} Contrary to expectations, increasing the number of retrieved passages from 1 to 10 did not consistently improve response quality. The top-1 approach often outperformed both top-10 variants, suggesting that retrieval quality (finding the most relevant single document) may be more important than retrieval quantity. The order of passages had mixed effects, with shuffled order sometimes providing different but not necessarily better information.

\section{Problem 4: Use of Generative AI}

\textbf{Question 4.1:} [To be filled if applicable]

\textbf{Question 4.2:} [To be filled if applicable]

\textbf{Question 4.3:} [To be filled if applicable]

\end{document}

